import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions.categorical import Categorical
import time
import numpy as np
# import clip
import math
from einops import rearrange, repeat
from functools import partial

# from models.transformer.tools import *

# from tools import *
from .motion_encoder import *
from .emage import *
from .kst_tools import *
import copy


# from .transformer import CAG
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)


def check_nan_inf(tensor, name="Tensor"):
    if torch.isnan(tensor).any():
        raise ValueError(f"{name} contains NaN values!")
    if torch.isinf(tensor).any():
        raise ValueError(f"{name} contains Inf values!")


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class predict_residual_zq(nn.Module):

    def __init__(self,
                 latent_dim=256,
                 num_head=8,
                 ffn_dim=1024,
                 dropout=0.1,
                 n_tokens=5):
        super(predict_residual_zq, self).__init__()
        self.cross_layer = nn.TransformerDecoderLayer(d_model=latent_dim,
                                                      nhead=num_head,
                                                      dim_feedforward=ffn_dim,
                                                      dropout=dropout)
        self.cross_attn_1 = nn.TransformerDecoder(self.cross_layer,
                                                  num_layers=3)
        self.cross_attn_2 = nn.TransformerDecoder(self.cross_layer,
                                                  num_layers=3)
        self.cross_attn_3 = nn.TransformerDecoder(self.cross_layer,
                                                  num_layers=3)
        self.cross_attn_4 = nn.TransformerDecoder(self.cross_layer,
                                                  num_layers=3)
        self.cross_attn_5 = nn.TransformerDecoder(self.cross_layer,
                                                  num_layers=3)
        self.map_zq2 = nn.Linear(latent_dim * 2, latent_dim)
        self.map_zq3 = nn.Linear(latent_dim * 3, latent_dim)
        self.map_zq4 = nn.Linear(latent_dim * 4, latent_dim)
        self.map_zq5 = nn.Linear(latent_dim * 5, latent_dim)
        self.classfier_1 = MLP(latent_dim, latent_dim, latent_dim)
        self.classfier_2 = MLP(latent_dim, latent_dim, latent_dim)
        self.classfier_3 = MLP(latent_dim, latent_dim, latent_dim)
        self.classfier_4 = MLP(latent_dim, latent_dim, latent_dim)
        self.classfier_5 = MLP(latent_dim, latent_dim, latent_dim)

    def forward(self, pre_index, cond):
        pre_index = pre_index.permute(1, 0, 2)
        cond = cond.permute(1, 0, 2)
        zq_1 = self.cross_attn_1(tgt=pre_index, memory=cond)
        zq_1 = zq_1 + pre_index
        zq_index_1 = self.classfier_1(zq_1.permute(1, 0, 2))

        pre_zq2 = torch.cat([pre_index, zq_1], dim=-1)
        # pre_zq2 = pre_zq2.permute(1,0,2)
        pre_zq2 = self.map_zq2(pre_zq2)
        # pre_zq2 = pre_zq2.permute(1,0,2)

        zq_2 = self.cross_attn_2(tgt=pre_zq2, memory=cond)
        zq_2 = zq_2 + pre_zq2

        zq_index_2 = self.classfier_2(zq_2.permute(1, 0, 2))

        pre_zq3 = torch.cat([pre_index, zq_1, zq_2], dim=-1)
        # pre_zq3 = pre_zq3.permute(1,0,2)
        pre_zq3 = self.map_zq3(pre_zq3)
        # pre_zq3 = pre_zq3.permute(1,0,2)
        zq_3 = self.cross_attn_3(tgt=pre_zq3, memory=cond)
        zq_3 = zq_3 + pre_zq3
        zq_index_3 = self.classfier_3(zq_3.permute(1, 0, 2))

        pre_zq4 = torch.cat([pre_index, zq_1, zq_2, zq_3], dim=-1)
        # pre_zq4 = pre_zq4.permute(1,0,2)
        pre_zq4 = self.map_zq4(pre_zq4)
        # pre_zq4 = pre_zq4.permute(1,0,2)
        zq_4 = self.cross_attn_4(tgt=pre_zq4, memory=cond)
        zq_4 = zq_4 + pre_zq4
        zq_index_4 = self.classfier_4(zq_4.permute(1, 0, 2))

        pre_zq5 = torch.cat([pre_index, zq_1, zq_2, zq_3, zq_4], dim=-1)
        # pre_zq5 = pre_zq5.permute(1,0,2)
        pre_zq5 = self.map_zq5(pre_zq5)
        # pre_zq5 = pre_zq5.permute(1,0,2)
        zq_5 = self.cross_attn_5(tgt=pre_zq5, memory=cond)
        zq_5 = zq_5 + pre_zq5
        zq_index_5 = self.classfier_5(zq_5.permute(1, 0, 2))

        zq_1 = zq_1.permute(1, 0, 2)
        zq_2 = zq_2.permute(1, 0, 2)
        zq_3 = zq_3.permute(1, 0, 2)
        zq_4 = zq_4.permute(1, 0, 2)
        zq_5 = zq_5.permute(1, 0, 2)
        return zq_1, zq_2, zq_3, zq_4, zq_5, zq_index_1, zq_index_2, zq_index_3, zq_index_4, zq_index_5


class RhythmicIdentificationLoss(nn.Module):

    def __init__(self, temperature=0.1):
        super(RhythmicIdentificationLoss, self).__init__()
        self.temperature = temperature

    def forward(self, facial_features, audio_features):
        """
        facial_features: Tensor of shape (batch_size, num_frames, feature_dim)
        audio_features: Tensor of shape (batch_size, num_frames, feature_dim)
        """
        # Normalize the features to compute cosine similarity
        # bs, t, c = facial_features.shape
        audio_features = F.avg_pool1d(audio_features.permute(0, 2, 1),
                                      kernel_size=4).permute(0, 2, 1)
        facial_features = F.normalize(facial_features, p=2, dim=-1)
        audio_features = F.normalize(audio_features, p=2, dim=-1)

        # Compute cosine similarity between each pair of facial and audio features
        similarity_matrix = torch.matmul(facial_features,
                                         audio_features.transpose(
                                             -1, -2)) / self.temperature

        # Create labels: each frame should correspond to itself (diagonal alignment)
        batch_size, num_frames, _ = facial_features.shape
        labels = torch.arange(num_frames).unsqueeze(0).repeat(
            batch_size, 1).to(facial_features.device)

        # Compute the InfoNCE loss
        loss = F.cross_entropy(similarity_matrix, labels)

        return loss


class RhythmicIdentificationLoss_hands(nn.Module):

    def __init__(self, temperature=0.1):
        super(RhythmicIdentificationLoss_hands, self).__init__()
        self.temperature = temperature

    def forward(self, facial_features, audio_features):
        """
        facial_features: Tensor of shape (batch_size, num_frames, feature_dim)
        audio_features: Tensor of shape (batch_size, num_frames, feature_dim)
        """
        # Normalize the features to compute cosine similarity
        # bs, t, c = facial_features.shape
        audio_features = F.avg_pool1d(audio_features.permute(0, 2, 1),
                                      kernel_size=2).permute(0, 2, 1)
        facial_features = F.normalize(facial_features, p=2, dim=-1)
        audio_features = F.normalize(audio_features, p=2, dim=-1)

        # Compute cosine similarity between each pair of facial and audio features
        similarity_matrix = torch.matmul(facial_features,
                                         audio_features.transpose(
                                             -1, -2)) / self.temperature

        # Create labels: each frame should correspond to itself (diagonal alignment)
        batch_size, num_frames, _ = facial_features.shape
        labels = torch.arange(num_frames).unsqueeze(0).repeat(
            batch_size, 1).to(facial_features.device)

        # Compute the InfoNCE loss
        loss = F.cross_entropy(similarity_matrix, labels)

        return loss


class mm_all_zq_behu(nn.Module):

    def __init__(self, args=None):
        super(mm_all_zq_behu, self).__init__()
        self.args = args
        # with open(f"{args.data_path}weights/vocab.pkl", 'rb') as f:
        #     self.lang_model = pickle.load(f)
        #     pre_trained_embedding = self.lang_model.word_embedding_weights
        ######## constractive loss ########
        self.hubert_cons_loss = RhythmicIdentificationLoss(temperature=0.1)
        self.hubert_face_cons_loss = RhythmicIdentificationLoss(
            temperature=0.1)
        self.beat_cons_loss = RhythmicIdentificationLoss(temperature=0.1)
        self.beat_face_cons_loss = RhythmicIdentificationLoss(temperature=0.1)
        # self.hubert_cons_loss = Recycle_loss()
        # self.beat_cons_loss = Recycle_loss()
        self.face_cons_mlp = MLP(256, args.hidden_size, 256)
        self.hands_cons_mlp = MLP(256, args.hidden_size, 256)

        self.hubert_encoder = nn.Sequential(*[
            nn.Conv1d(1024, 256, 3, 1, 1, bias=False),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.Conv1d(256, 256, 3, 1, 1, bias=False)
        ])
        self.hubert_encoder_body = nn.Sequential(*[
            nn.Conv1d(1024, 256, 3, 1, 1, bias=False),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.Conv1d(256, 256, 3, 1, 1, bias=False)
        ])
        ##### predict_residual #####
        self.predict_res_face = predict_residual_zq(latent_dim=256,
                                                    num_head=8,
                                                    ffn_dim=1024,
                                                    dropout=0.1,
                                                    n_tokens=5)
        self.predict_res_hands = predict_residual_zq(latent_dim=256,
                                                     num_head=8,
                                                     ffn_dim=1024,
                                                     dropout=0.1,
                                                     n_tokens=5)
        self.predict_res_upper = predict_residual_zq(latent_dim=256,
                                                     num_head=8,
                                                     ffn_dim=1024,
                                                     dropout=0.1,
                                                     n_tokens=5)
        self.predict_res_lower = predict_residual_zq(latent_dim=256,
                                                     num_head=8,
                                                     ffn_dim=1024,
                                                     dropout=0.1,
                                                     n_tokens=5)

        ##### con1d #####
        self.face1d = nn.Conv1d(in_channels=256,
                                out_channels=256,
                                kernel_size=2,
                                stride=2)
        self.face1d_2 = nn.Conv1d(in_channels=256,
                                  out_channels=256,
                                  kernel_size=2,
                                  stride=2)
        self.body1d = nn.Conv1d(in_channels=768,
                                out_channels=768,
                                kernel_size=2,
                                stride=2)
        self.upper1d = nn.Conv1d(in_channels=256,
                                 out_channels=256,
                                 kernel_size=2,
                                 stride=2)
        self.hands1d = nn.Conv1d(in_channels=256,
                                 out_channels=256,
                                 kernel_size=2,
                                 stride=2)
        self.lower1d = nn.Conv1d(in_channels=256,
                                 out_channels=256,
                                 kernel_size=2,
                                 stride=2)
        self.spearker_encoder_body1d = nn.Conv1d(in_channels=768,
                                                 out_channels=768,
                                                 kernel_size=2,
                                                 stride=2)
        self.face_latent_mlp = MLP(256, args.hidden_size, 256)

        # self.text_pre_encoder_face = nn.Embedding.from_pretrained(torch.FloatTensor(pre_trained_embedding),freeze=args.t_fix_pre)
        # self.text_encoder_face = nn.Linear(300, args.audio_f)
        # self.text_encoder_face = nn.Linear(300, args.audio_f)
        # self.text_pre_encoder_body = nn.Embedding.from_pretrained(torch.FloatTensor(pre_trained_embedding),freeze=args.t_fix_pre)
        # self.text_encoder_body = nn.Linear(300, args.audio_f) # audio_f = 256
        # self.text_encoder_body = nn.Linear(300, args.audio_f) # audio_f = 256
        # self.audio_pre_encoder_face = WavEncoder(args.audio_f, audio_in=2)
        # self.audio_pre_encoder_body = WavEncoder(args.audio_f, audio_in=2)
        self.audio_pre_encoder_face = MLP(3, args.hidden_size, 256)
        self.audio_pre_encoder_body = MLP(3, args.hidden_size, 256)
        self.at_attn_face = nn.Linear(args.audio_f * 2, args.audio_f * 2)
        self.at_attn_body = nn.Linear(args.audio_f * 2, args.audio_f * 2)
        args_top = copy.deepcopy(self.args)
        args_top.vae_layer = 3
        args_top.vae_length = args.motion_f  # args.motion_f 256
        args_top.vae_test_dim = args.pose_dims + 3 + 4  # 330 + 3 + 4 = 337
        self.motion_encoder = VQEncoderV6(
            args_top)  # masked motion to latent bs t 337 to bs t 256

        # face decoder  hidden_size:768, audio_f:256, vae_codebook_size:256
        self.feature2face = nn.Linear(args.audio_f * 2,
                                      args.hidden_size)  # 256*2 to 768
        self.face2latent = nn.Linear(
            args.hidden_size, args.vae_codebook_size)  # vae_codebook_size:256
        self.transformer_de_layer = nn.TransformerDecoderLayer(
            d_model=self.args.hidden_size,
            nhead=4,
            dim_feedforward=self.args.hidden_size * 2,
            batch_first=True)
        self.transformer_de_fu_layer = nn.TransformerDecoderLayer(
            d_model=256, nhead=4, dim_feedforward=256 * 2, batch_first=True)
        self.hands_face_decoder = nn.TransformerDecoder(
            self.transformer_de_fu_layer, num_layers=1)
        self.face_hands_decoder = nn.TransformerDecoder(
            self.transformer_de_fu_layer, num_layers=1)

        self.face_decoder = nn.TransformerDecoder(self.transformer_de_layer,
                                                  num_layers=4)
        # pose_length = 64
        self.position_embeddings = PeriodicPositionalEncoding(
            self.args.hidden_size,
            period=self.args.pose_length,
            max_seq_len=self.args.pose_length)

        # motion decoder
        self.transformer_en_layer = nn.TransformerEncoderLayer(
            d_model=self.args.hidden_size,
            nhead=4,
            dim_feedforward=self.args.hidden_size * 2,
            batch_first=True)
        self.motion_self_encoder = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=1)
        self.audio_feature2motion = nn.Linear(args.audio_f,
                                              args.hidden_size)  # 256 to 768
        self.feature2motion = nn.Linear(args.motion_f,
                                        args.hidden_size)  # 256 to 768

        self.bodyhints_face = MLP(args.motion_f, args.hidden_size,
                                  args.motion_f)  # 256 to 256
        self.bodyhints_body = MLP(args.motion_f, args.hidden_size,
                                  args.motion_f)  # 256 to 256
        self.motion2latent_upper = MLP(args.hidden_size, args.hidden_size,
                                       self.args.hidden_size)
        self.motion2latent_hands = MLP(args.hidden_size, args.hidden_size,
                                       self.args.hidden_size)
        self.motion2latent_lower = MLP(args.hidden_size, args.hidden_size,
                                       self.args.hidden_size)
        self.wordhints_decoder = nn.TransformerDecoder(
            self.transformer_de_layer, num_layers=8)

        self.upper_decoder = nn.TransformerDecoder(self.transformer_de_layer,
                                                   num_layers=1)
        self.hands_decoder = nn.TransformerDecoder(self.transformer_de_layer,
                                                   num_layers=1)
        self.lower_decoder = nn.TransformerDecoder(self.transformer_de_layer,
                                                   num_layers=1)

        self.upper_hands_decoder = nn.TransformerDecoder(
            self.transformer_de_fu_layer, num_layers=1)
        self.lower_hands_decoder = nn.TransformerDecoder(
            self.transformer_de_fu_layer, num_layers=1)

        self.face_classifier = MLP(self.args.vae_codebook_size,
                                   args.hidden_size,
                                   self.args.vae_codebook_size)
        self.upper_classifier = MLP(self.args.vae_codebook_size,
                                    args.hidden_size,
                                    self.args.vae_codebook_size)
        self.hands_classifier = MLP(self.args.vae_codebook_size,
                                    args.hidden_size,
                                    self.args.vae_codebook_size)
        self.lower_classifier = MLP(self.args.vae_codebook_size,
                                    args.hidden_size,
                                    self.args.vae_codebook_size)

        self.mask_embeddings = nn.Parameter(
            torch.zeros(1, 1, self.args.pose_dims + 3 + 4))  # [1, 1, 337]
        self.motion_down_upper = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_hands = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_lower = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_upper = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_hands = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_lower = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self._reset_parameters()

        self.spearker_encoder_body = nn.Embedding(25, args.hidden_size)
        self.spearker_encoder_face = nn.Embedding(25, args.hidden_size)

    def _reset_parameters(self):
        nn.init.normal_(self.mask_embeddings, 0, self.args.hidden_size**-0.5)

    def forward(self,
                in_audio=None,
                in_word=None,
                mask=None,
                is_test=None,
                in_motion=None,
                use_attentions=True,
                use_word=True,
                in_id=None,
                hubert=None):
        # print('in_audio:', in_audio.shape)
        # print('in_word:', in_word.shape)

        # in_word_face = self.text_pre_encoder_face(in_word)
        # in_word_face = self.text_encoder_face(in_word_face)
        # in_word_body = self.text_pre_encoder_body(in_word)
        in_word_face = self.hubert_encoder(hubert.permute(0, 2,
                                                          1)).permute(0, 2, 1)
        in_word_body = self.hubert_encoder_body(hubert.permute(0, 2,
                                                               1)).permute(
                                                                   0, 2, 1)
        # in_word_body = self.text_encoder_body(in_word_body)
        bs, t, c = in_word_face.shape
        in_audio_face = self.audio_pre_encoder_face(in_audio)  # [bs, t, 256]
        in_audio_body = self.audio_pre_encoder_body(in_audio)  # [bs, t, 256]
        # print('in_audio_body:', in_audio_body.shape)
        # if in_audio_face.shape[1] != in_motion.shape[1]:  # in_motion [bs, t, 337]
        #     diff_length = in_motion.shape[1]- in_audio_face.shape[1]
        #     if diff_length < 0:
        #         in_audio_face = in_audio_face[:, :diff_length, :]
        #         in_audio_body = in_audio_body[:, :diff_length, :]
        #     else:
        #         in_audio_face = torch.cat((in_audio_face, in_audio_face[:,-diff_length:]),1)
        #         in_audio_body = torch.cat((in_audio_body, in_audio_body[:,-diff_length:]),1)

        if use_attentions:
            alpha_at_face = torch.cat([in_word_face, in_audio_face],
                                      dim=-1).reshape(bs, t, c * 2)
            alpha_at_face = self.at_attn_face(alpha_at_face).reshape(
                bs, t, c, 2)  # bs, t, c, 2
            alpha_at_face = alpha_at_face.softmax(dim=-1)
            fusion_face = in_word_face * alpha_at_face[:, :, :,
                                                       1] + in_audio_face * alpha_at_face[:, :, :,
                                                                                          0]
            alpha_at_body = torch.cat([in_word_body, in_audio_body],
                                      dim=-1).reshape(bs, t, c * 2)
            alpha_at_body = self.at_attn_body(alpha_at_body).reshape(
                bs, t, c, 2)
            alpha_at_body = alpha_at_body.softmax(dim=-1)
            fusion_body = in_word_body * alpha_at_body[:, :, :,
                                                       1] + in_audio_body * alpha_at_body[:, :, :,
                                                                                          0]
        else:
            fusion_face = in_word_face + in_audio_face
            fusion_body = in_word_body + in_audio_body
        # print('fusion_face:', fusion_face.shape)

        masked_embeddings = self.mask_embeddings.expand_as(
            in_motion)  # in_motion [bs, t, 337]
        # mask [bs, t, 337], 前四帧为0，后面为1
        masked_motion = torch.where(mask == 1, masked_embeddings, in_motion)
        body_hint = self.motion_encoder(masked_motion)  # bs t 256
        # print('id:', in_id.shape)

        speaker_embedding_face = self.spearker_encoder_face(in_id).squeeze(
            2)  # bs, t, 768
        # print('speaker_embedding_face:', speaker_embedding_face.shape)
        # print('speaker_embedding_face:', speaker_embedding_face.shape)
        speaker_embedding_body = self.spearker_encoder_body(in_id).squeeze(2)

        # decode face
        use_body_hints = True
        if use_body_hints:
            body_hint_face = self.bodyhints_face(body_hint)
            fusion_face_a = torch.cat([fusion_face, body_hint_face], dim=2)
        a2g_face = self.feature2face(fusion_face_a)
        face_embeddings = speaker_embedding_face
        face_embeddings = self.position_embeddings(face_embeddings)
        # print('face_embeddings:', face_embeddings.shape)
        decoded_face = self.face_decoder(tgt=face_embeddings, memory=a2g_face)
        face_latent = self.face2latent(decoded_face)
        face_latent = face_latent.permute(0, 2, 1)
        face_latent = self.face1d(face_latent)
        face_latent = face_latent.permute(0, 2, 1)
        face_latent = self.face_latent_mlp(face_latent)
        face_latent = face_latent.permute(0, 2, 1)
        face_latent = self.face1d_2(face_latent)
        # face_latent_cls = face_latent.permute(0, 2, 1)
        # print('face_latent_cls:', face_latent_cls.shape)
        # exit()
        face_latent_prezq = face_latent.permute(0, 2, 1)
        ######### hubert cons loss ###########
        hubert_cons_loss = self.hubert_face_cons_loss(face_latent_prezq,
                                                      in_word_face)
        # beat_face_cons_loss = self.beat_face_cons_loss(face_latent_prezq, in_audio_face)
        # face_latent_prezq = self.face_cons_mlp(face_latent_prezq)

        body_hint_body = self.bodyhints_body(body_hint)  # MLP 256 to 256
        motion_embeddings = self.feature2motion(
            body_hint_body)  # linear 256 to 768
        motion_embeddings = speaker_embedding_body + motion_embeddings  # bs, t, 768
        motion_embeddings = self.position_embeddings(
            motion_embeddings)  # bs, t, 768

        # bi-directional self-attention
        motion_refined_embeddings = self.motion_self_encoder(motion_embeddings)

        # audio to gesture cross-modal attention
        if use_word:
            a2g_motion = self.audio_feature2motion(
                fusion_body)  # linear 256 to 768
            motion_refined_embeddings_in = motion_refined_embeddings + speaker_embedding_body  # bs, t, 768
            motion_refined_embeddings_in = self.position_embeddings(
                motion_refined_embeddings)
            word_hints = self.wordhints_decoder(
                tgt=motion_refined_embeddings_in, memory=a2g_motion)
            motion_refined_embeddings = motion_refined_embeddings + word_hints

        # feedforward
        motion_refined_embeddings = motion_refined_embeddings.permute(0, 2, 1)
        motion_refined_embeddings = self.body1d(motion_refined_embeddings)
        motion_refined_embeddings = motion_refined_embeddings.permute(0, 2, 1)
        speaker_embedding_body = speaker_embedding_body.permute(0, 2, 1)
        speaker_embedding_body = self.spearker_encoder_body1d(
            speaker_embedding_body)
        speaker_embedding_body = speaker_embedding_body.permute(0, 2, 1)

        upper_latent = self.motion2latent_upper(motion_refined_embeddings)
        hands_latent = self.motion2latent_hands(motion_refined_embeddings)
        lower_latent = self.motion2latent_lower(motion_refined_embeddings)

        upper_latent_in = upper_latent + speaker_embedding_body
        upper_latent_in = self.position_embeddings(upper_latent_in)
        hands_latent_in = hands_latent + speaker_embedding_body
        hands_latent_in = self.position_embeddings(hands_latent_in)
        lower_latent_in = lower_latent + speaker_embedding_body
        lower_latent_in = self.position_embeddings(lower_latent_in)

        # transformer decoder
        motion_upper = self.upper_decoder(tgt=upper_latent_in,
                                          memory=hands_latent + lower_latent)
        motion_hands = self.hands_decoder(tgt=hands_latent_in,
                                          memory=upper_latent + lower_latent)
        motion_lower = self.lower_decoder(tgt=lower_latent_in,
                                          memory=upper_latent + hands_latent)
        upper_latent = self.motion_down_upper(
            motion_upper + upper_latent)  # linear 768 to 256
        hands_latent = self.motion_down_hands(motion_hands + hands_latent)
        lower_latent = self.motion_down_lower(motion_lower + lower_latent)

        upper_latent = upper_latent.permute(0, 2, 1)
        upper_latent = self.upper1d(upper_latent)
        upper_latent_prezq = upper_latent.permute(0, 2, 1)

        hands_latent = hands_latent.permute(0, 2, 1)
        hands_latent = self.hands1d(hands_latent)
        hands_latent_prezq = hands_latent.permute(0, 2, 1)

        ########## beat cons loss ###########
        beat_cons_loss = self.beat_cons_loss(hands_latent_prezq, in_word_body)
        # hubert_cons_loss = self.hubert_cons_loss(hands_latent_prezq, in_word_body )
        # beat_cons_loss = self.beat_cons_loss(hands_latent_prezq, in_audio_body)

        # hands_latent_prezq = self.hands_cons_mlp(hands_latent_prezq)

        lower_latent = lower_latent.permute(0, 2, 1)
        lower_latent = self.lower1d(lower_latent)
        lower_latent_prezq = lower_latent.permute(0, 2, 1)

        hands_latent = self.hands_face_decoder(tgt=hands_latent_prezq,
                                               memory=face_latent_prezq)
        face_latent = self.face_hands_decoder(tgt=face_latent_prezq,
                                              memory=hands_latent_prezq)
        # face_latent = face_latent_prezq
        upper_latent = self.upper_hands_decoder(tgt=upper_latent_prezq,
                                                memory=hands_latent +
                                                lower_latent_prezq)
        lower_latent = self.lower_hands_decoder(tgt=lower_latent_prezq,
                                                memory=upper_latent_prezq +
                                                hands_latent)
        # face_latent = face_latent_prezq
        # upper_latent = upper_latent_prezq
        # lower_latent = lower_latent_prezq
        # hands_latent = hands_latent_prezq

        # ########### beat cons loss ###########
        # beat_cons_loss = self.beat_cons_loss(hands_latent, in_word_body)
        # ########## hubert cons loss ###########
        # hubert_cons_loss = self.hubert_cons_loss(face_latent, in_word_face)

        zq_index0_lower = self.lower_classifier(lower_latent)
        # zq0_lower = self.lower_classifier(lower_latent)
        zq_index0_face = self.face_classifier(face_latent)  # bs, t, 256
        # zq0_face = self.face_classifier(face_latent) # bs, t, 256

        zq1_face, zq2_face, zq3_face, zq4_face, zq5_face, zq_index1_face, zq_index2_face, zq_index3_face, zq_index4_face, zq_index5_face = self.predict_res_face(
            face_latent, fusion_face)
        # motion spatial encoder
        zq1_lower, zq2_lower, zq3_lower, zq4_lower, zq5_lower, zq_index1_lower, zq_index2_lower, zq_index3_lower, zq_index4_lower, zq_index5_lower = self.predict_res_lower(
            lower_latent, fusion_body)

        zq_index0_upper = self.upper_classifier(upper_latent)
        # zq0_upper = self.upper_classifier(upper_latent)
        zq1_upper, zq2_upper, zq3_upper, zq4_upper, zq5_upper, zq_index1_upper, zq_index2_upper, zq_index3_upper, zq_index4_upper, zq_index5_upper = self.predict_res_upper(
            upper_latent, fusion_body)

        zq_index0_hands = self.hands_classifier(hands_latent)
        # zq0_hands = self.hands_classifier(hands_latent)
        zq1_hands, zq2_hands, zq3_hands, zq4_hands, zq5_hands, zq_index1_hands, zq_index2_hands, zq_index3_hands, zq_index4_hands, zq_index5_hands = self.predict_res_hands(
            hands_latent, fusion_body)

        cls_face = torch.stack([
            zq_index0_face, zq_index1_face, zq_index2_face, zq_index3_face,
            zq_index4_face, zq_index5_face
        ],
                               dim=-1)
        # cls_face = zq_index0_face

        cls_upper = torch.stack([
            zq_index0_upper, zq_index1_upper, zq_index2_upper, zq_index3_upper,
            zq_index4_upper, zq_index5_upper
        ],
                                dim=-1)
        cls_lower = torch.stack([
            zq_index0_lower, zq_index1_lower, zq_index2_lower, zq_index3_lower,
            zq_index4_lower, zq_index5_lower
        ],
                                dim=-1)
        cls_hands = torch.stack([
            zq_index0_hands, zq_index1_hands, zq_index2_hands, zq_index3_hands,
            zq_index4_hands, zq_index5_hands
        ],
                                dim=-1)

        # rec_face = face_latent.unsqueeze(2)
        rec_face = torch.stack(
            [face_latent, zq1_face, zq2_face, zq3_face, zq4_face, zq5_face],
            dim=1).unsqueeze(2)

        rec_upper = torch.stack([
            upper_latent, zq1_upper, zq2_upper, zq3_upper, zq4_upper, zq5_upper
        ],
                                dim=1).unsqueeze(2)
        rec_lower = torch.stack([
            lower_latent, zq1_lower, zq2_lower, zq3_lower, zq4_lower, zq5_lower
        ],
                                dim=1).unsqueeze(2)
        rec_hands = torch.stack([
            hands_latent, zq1_hands, zq2_hands, zq3_hands, zq4_hands, zq5_hands
        ],
                                dim=1).unsqueeze(2)
        # hubert_cons_loss = hubert_cons_loss + hubert_face_cons_loss
        # beat_cons_loss = beat_cons_loss + beat_face_cons_loss
        return {
            'hubert_cons_loss': hubert_cons_loss,
            'beat_cons_loss': beat_cons_loss,
            "rec_face": rec_face,
            "rec_upper": rec_upper,
            "rec_lower": rec_lower,
            "rec_hands": rec_hands,
            # "rec_face":face_latent,
            # "rec_upper":upper_latent,
            # "rec_lower":lower_latent,
            # "rec_hands":hands_latent,
            "cls_face": cls_face,
            "cls_upper": cls_upper,
            "cls_lower": cls_lower,
            "cls_hands": cls_hands,
        }


def _clone_module(module, N):
    # 用于复制 N 份相同的层
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


class MyTransformerDecoderLayer(nn.TransformerDecoderLayer):

    def forward(self,
                tgt,
                memory,
                tgt_mask=None,
                memory_mask=None,
                tgt_key_padding_mask=None,
                memory_key_padding_mask=None,
                need_weights=False):
        x = tgt

        # 1. 自注意力
        # 这里的 need_weights=False 是因为通常只需要 cross-attention 的权重
        x2 = self.self_attn(x,
                            x,
                            x,
                            attn_mask=tgt_mask,
                            key_padding_mask=tgt_key_padding_mask,
                            need_weights=False)[0]
        x = x + self.dropout1(x2)
        x = self.norm1(x)

        # 2. 交叉注意力
        x2, cross_attn_weights = self.multihead_attn(
            x,
            memory,
            memory,
            attn_mask=memory_mask,
            key_padding_mask=memory_key_padding_mask,
            need_weights=need_weights)
        x = x + self.dropout2(x2)
        x = self.norm2(x)

        # 3. 前馈网络
        x2 = self.linear2(self.dropout(self.activation(self.linear1(x))))
        x = x + self.dropout3(x2)
        x = self.norm3(x)

        # 返回输出和交叉注意力权重
        if need_weights:
            return x, cross_attn_weights
        else:
            return x, None


class MyTransformerDecoder(nn.Module):

    def __init__(self, decoder_layer, num_layers, norm=None):
        super().__init__()
        self.layers = _clone_module(decoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm

    def forward(self,
                tgt,
                memory,
                tgt_mask=None,
                memory_mask=None,
                tgt_key_padding_mask=None,
                memory_key_padding_mask=None,
                need_weights=False):
        output = tgt
        all_weights = []

        for layer in self.layers:
            output, cross_attn_weights = layer(
                output,
                memory,
                tgt_mask=tgt_mask,
                memory_mask=memory_mask,
                tgt_key_padding_mask=tgt_key_padding_mask,
                memory_key_padding_mask=memory_key_padding_mask,
                need_weights=need_weights)
            if need_weights and cross_attn_weights is not None:
                all_weights.append(cross_attn_weights)

        if self.norm is not None:
            output = self.norm(output)

        if need_weights:
            # 形状会是 (num_layers, batch_size, nhead, tgt_len, src_len)
            return output, torch.stack(all_weights, dim=0)
        else:
            return output


# ----------------- Soft2HardMaskSTE 定义 ----------------- #
class Soft2HardMaskSTE(torch.autograd.Function):

    @staticmethod
    def forward(ctx, importance_scores, mask_ratio, hard_ratio, soft_ratio):
        # importance_scores: (B, N)
        B, N = importance_scores.shape
        mask = torch.zeros_like(importance_scores)
        for b in range(B):
            scores = importance_scores[b]  # (N,)
            sorted_vals, sorted_idx = scores.sort(descending=True)
            k_total = int(round(mask_ratio * N))
            k_hard = int(round(hard_ratio * N))
            k_soft = int(round(soft_ratio * N))
            k_random = k_total - k_hard - k_soft
            if k_random < 0:
                k_random = 0
            # Hard mask：直接选择前 k_hard 个
            hard_idx = sorted_idx[:k_hard]
            # Soft mask：在接下来的候选区域中采样 k_soft 个
            if k_soft > 0 and (k_hard + k_soft) <= N:
                soft_candidates = sorted_idx[k_hard:k_hard + k_soft]
                soft_scores = scores[soft_candidates].float()
                # 平移 soft_scores 保证非负
                shifted_scores = soft_scores - soft_scores.min() + 1e-6
                if shifted_scores.sum() > 0:
                    p = shifted_scores / shifted_scores.sum()
                else:
                    p = torch.ones_like(
                        shifted_scores) / shifted_scores.numel()
                soft_idx = torch.multinomial(p, k_soft, replacement=False)
                soft_idx = soft_candidates[soft_idx]
            else:
                soft_idx = torch.tensor([],
                                        dtype=torch.long,
                                        device=importance_scores.device)
            # Random mask：在剩余部分随机采样 k_random 个
            remaining = sorted_idx[k_hard + (k_soft if k_soft > 0 else 0):]
            if k_random > 0 and remaining.numel() > 0:
                perm = torch.randperm(remaining.numel(),
                                      device=importance_scores.device)
                random_idx = remaining[perm[:k_random]]
            else:
                random_idx = torch.tensor([],
                                          dtype=torch.long,
                                          device=importance_scores.device)
            final_idx = torch.cat([hard_idx, soft_idx, random_idx], dim=0)
            mask[b, final_idx] = 1.0
        return mask

    @staticmethod
    def backward(ctx, grad_output):
        # 直接传递梯度（STE）
        return grad_output, None, None, None


# ----------------- FiLM 模块定义 ----------------- #
class FiLM(nn.Module):
    """
    FiLM 层：从条件信号生成 gamma 和 beta，对输入特征进行调制。
    condition: (B, T_q, embed_dim) ——先对时间维度求平均，得到 (B, embed_dim)
    feature: (B, T, embed_dim)
    """

    def __init__(self, embed_dim):
        super(FiLM, self).__init__()
        self.gamma_fc = nn.Linear(embed_dim, embed_dim)
        self.beta_fc = nn.Linear(embed_dim, embed_dim)

    def forward(self, condition, feature):
        cond_repr = condition.mean(dim=1)  # (B, embed_dim)
        gamma = self.gamma_fc(cond_repr).unsqueeze(1)  # (B, 1, embed_dim)
        beta = self.beta_fc(cond_repr).unsqueeze(1)  # (B, 1, embed_dim)
        modulated_feature = gamma * feature + beta
        return modulated_feature


# ----------------- 多条件融合 Mask Generator（采用 FiLM + soft2hard 策略） ----------------- #
class MaskGeneratorMultiConditionFusionSTE(nn.Module):

    def __init__(self,
                 motion_dim=337,
                 embed_dim=256,
                 frame_down_factor=4,
                 nhead=4,
                 num_layers=1):
        """
        利用共享 transformer decoder 提取共性信息，再通过 FiLM 动态调节注入各条件独特信息，
        最后采用 soft2hard 策略生成 mask。
        参数：
          mask_ratio_*：总 mask 比例；
          hard_ratio_*：硬 mask 比例；
          soft_ratio_*：软 mask 比例；
          剩余部分将随机 mask 掉。
        """
        super(MaskGeneratorMultiConditionFusionSTE, self).__init__()
        self.frame_down_factor = frame_down_factor

        self.embed_dim = embed_dim
        # 帧级部分：对 motion 求平均后投影
        self.motion_proj_frame = nn.Linear(motion_dim, embed_dim)
        # 共享 transformer decoder 提取共性 attention map
        decoder_layer_shared = MyTransformerDecoderLayer(d_model=embed_dim,
                                                         nhead=nhead,
                                                         batch_first=True)
        self.shared_transformer_decoder = MyTransformerDecoder(
            decoder_layer_shared, num_layers=num_layers)
        # 条件特定 FiLM 层
        self.film_layers = nn.ModuleDict({
            key: FiLM(embed_dim)
            for key in ["face", "hands", "upper", "lower"]
        })
        # 融合权重（可学习），平衡共性与独特信息
        self.fusion_weights = nn.Parameter(torch.ones(4) * 0.5)

        # Joint 级部分
        self.motion_proj_joint = nn.Linear(motion_dim, embed_dim)
        decoder_layer_joint = MyTransformerDecoderLayer(d_model=embed_dim,
                                                        nhead=nhead,
                                                        batch_first=True)
        self.shared_transformer_decoder_joint = MyTransformerDecoder(
            decoder_layer_joint, num_layers=num_layers)
        self.film_layers_joint = nn.ModuleDict({
            key: FiLM(embed_dim)
            for key in ["face", "hands", "upper", "lower"]
        })
        self.fusion_weights_joint = nn.Parameter(torch.ones(4) * 0.5)
    def get_attention_map(self, motion,
                conditions,
                need_weights=True):
        
        if need_weights:
            fused_attn_list = []
            out_attn_list = []
            for i, key in enumerate(["hands", "upper", "lower"]):
                cond = conditions[key]  # (B, T_q, embed_dim)
                # 共享 transformer decoder 得到共性 attention map
                out_attn, cross_attn_shared = self.shared_transformer_decoder(
                    cond, motion, need_weights=need_weights)
                # cross_attn_shared: (num_layers, B, tgt_len, src_len), tgt_len = T_q, src_len = T
                common_attn = cross_attn_shared.mean(dim=(0, 2))  # (B, T)
                # FiLM 对 memory_frame 进行条件特定调制
                unique_feature = self.film_layers[key](
                    cond, motion)  # (B, T, embed_dim)
                unique_attn = unique_feature.mean(dim=-1)  # (B, T)
                weight = self.fusion_weights[i]
                fused_attn = weight * common_attn + (
                    1 - weight) * unique_attn  # (B, T)
                fused_attn_list.append(fused_attn)
                out_attn_list.append(out_attn)
            # 融合各条件得到最终帧级重要性评分
            fused_attn_final = torch.stack(fused_attn_list,
                                           dim=0).mean(dim=0)  # (B, T)
            return fused_attn_final
    def forward(self, motion, conditions, mask_ratio_frame, hard_ratio_frame,
                soft_ratio_frame, mask_ratio_joint, hard_ratio_joint,
                soft_ratio_joint):
        """
        motion: (B, T, J, motion_dim)
        conditions: dict，包含 "face", "hands", "upper", "lower"，每个形状 (B, T_q, embed_dim)
          T_q = T // frame_down_factor
        返回：
          final_mask: (B, T, J, 1)
        """
        # B, T, J, motion_dim = motion.shape
        # ------------- 帧级 mask 计算 -------------
        # motion_avg = motion.mean(dim=2)  # (B, T, motion_dim)
        # memory_frame = self.motion_proj_frame(motion_avg)  # (B, T, embed_dim)
        memory_frame = self.motion_proj_frame(motion)  # (B, T, embed_dim)

        # memory_frame = motion

        fused_attn_list = []
        for i, key in enumerate(["face", "hands", "upper", "lower"]):
            cond = conditions[key]  # (B, T_q, embed_dim)
            # 共享 transformer decoder 得到共性 attention map
            _, cross_attn_shared = self.shared_transformer_decoder(
                cond, memory_frame, need_weights=True)
            # cross_attn_shared: (num_layers, B, tgt_len, src_len), tgt_len = T_q, src_len = T
            common_attn = cross_attn_shared.mean(dim=(0, 2))  # (B, T)
            # FiLM 对 memory_frame 进行条件特定调制
            unique_feature = self.film_layers[key](
                cond, memory_frame)  # (B, T, embed_dim)
            unique_attn = unique_feature.mean(dim=-1)  # (B, T)
            weight = self.fusion_weights[i]
            fused_attn = weight * common_attn + (
                1 - weight) * unique_attn  # (B, T)
            fused_attn_list.append(fused_attn)
        # 融合各条件得到最终帧级重要性评分
        fused_attn_final = torch.stack(fused_attn_list,
                                       dim=0).mean(dim=0)  # (B, T)
        mask_frame = Soft2HardMaskSTE.apply(fused_attn_final, mask_ratio_frame,
                                            hard_ratio_frame,
                                            soft_ratio_frame)  # (B, T)

        # ------------- Joint 级 mask 计算 -------------
        # motion_joint_flat = motion.view(B, T * J,
        #                                 motion_dim)  # (B, T*J, motion_dim)
        # memory_joint = self.motion_proj_joint(
        #     motion_joint_flat)  # (B, T*J, embed_dim)
        # fused_attn_list_joint = []
        # for i, key in enumerate(["face", "hands", "upper", "lower"]):
        #     cond = conditions[key]  # (B, T_q, embed_dim)
        #     _, cross_attn_shared_joint = self.shared_transformer_decoder_joint(
        #         cond, memory_joint, need_weights=True)
        #     # cross_attn_shared_joint: (num_layers, B, tgt_len, src_len), src_len = T*J
        #     common_attn_joint = cross_attn_shared_joint.mean(
        #         dim=(0, 2))  # (B, T*J)
        #     unique_feature_joint = self.film_layers_joint[key](
        #         cond, memory_joint)  # (B, T*J, embed_dim)
        #     unique_attn_joint = unique_feature_joint.mean(dim=-1)  # (B, T*J)
        #     weight_joint = self.fusion_weights_joint[i]
        #     fused_attn_joint = weight_joint * common_attn_joint + (
        #         1 - weight_joint) * unique_attn_joint  # (B, T*J)
        #     fused_attn_list_joint.append(fused_attn_joint)
        # fused_attn_final_joint = torch.stack(fused_attn_list_joint,
        #                                      dim=0).mean(dim=0)  # (B, T*J)
        # fused_attn_final_joint = fused_attn_final_joint.view(B, T,
        #                                                      J)  # (B, T, J)
        # mask_joint = Soft2HardMaskSTE.apply(
        #     fused_attn_final_joint.view(B, T * J), mask_ratio_joint,
        #     hard_ratio_joint, soft_ratio_joint)  # (B, T*J)
        # mask_joint = mask_joint.view(B, T, J)  # (B, T, J)

        # ------------- 合并帧级与 Joint 级 mask -------------
        mask_frame_exp = mask_frame.unsqueeze(-1)  # (B, T, 1)
        # final_mask = mask_frame_exp * mask_joint  # (B, T, J)
        final_mask = mask_frame_exp  # (B, T, J)
        # final_mask = final_mask.unsqueeze(-1)  # (B, T, J, 1)
        return final_mask


class JointDimReducer(nn.Module):

    def __init__(self, num_joints, in_dim=6, out_dim=6):
        """
        参数:
          num_joints: 关节数 J
          in_dim: 原始每个关节的特征维度（例如6）
          out_dim: 降维后的每个关节特征维度（可以与 in_dim 不同）
        """
        super(JointDimReducer, self).__init__()
        self.num_joints = num_joints
        # 为每个 joint 独立构造一个线性层
        self.linears = nn.ModuleList(
            [nn.Linear(in_dim, out_dim) for _ in range(num_joints)])

    def forward(self, x):
        """
        输入 x: (B, T, J, in_dim)
        输出: (B, T, J*out_dim)
        """
        outputs = []
        # 对每个 joint 分别进行降维
        for j in range(self.num_joints):
            # 选择第 j 个关节，形状为 (B, T, in_dim)
            joint_feature = x[:, :, j, :]
            # 对该 joint 特征进行线性变换，输出形状为 (B, T, out_dim)
            out = self.linears[j](joint_feature)
            # 在 joint 维度上增加一个维度 (B, T, 1, out_dim)
            outputs.append(out.unsqueeze(2))
        # 将所有 joint 的结果拼接起来，得到 (B, T, J, out_dim)
        reduced = torch.cat(outputs, dim=2)
        # 如果需要将 joint 维度与特征维度合并成一个长向量，可以展平 joint 维度：
        flattened = reduced.view(x.size(0), x.size(1), -1)
        return flattened


class mmm(nn.Module):

    def __init__(self, args=None):
        super(mmm, self).__init__()
        self.args = args
        ######## constractive loss ########
        self.hubert_cons_loss = RhythmicIdentificationLoss(temperature=0.1)
        self.hubert_face_cons_loss = RhythmicIdentificationLoss(
            temperature=0.1)
        self.beat_cons_loss = RhythmicIdentificationLoss(temperature=0.1)
        self.beat_face_cons_loss = RhythmicIdentificationLoss(temperature=0.1)
        # self.hubert_cons_loss = Recycle_loss()
        # self.beat_cons_loss = Recycle_loss()
        self.face_cons_mlp = MLP(256, args.hidden_size, 256)
        self.hands_cons_mlp = MLP(256, args.hidden_size, 256)

        self.hubert_encoder = nn.Sequential(*[
            nn.Conv1d(1024, 256, 3, 1, 1, bias=False),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.Conv1d(256, 256, 3, 1, 1, bias=False)
        ])
        self.hubert_encoder_body = nn.Sequential(*[
            nn.Conv1d(1024, 256, 3, 1, 1, bias=False),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.Conv1d(256, 256, 3, 1, 1, bias=False)
        ])
        ##### predict_residual #####
        self.predict_res_face = predict_residual_zq(latent_dim=256,
                                                    num_head=8,
                                                    ffn_dim=1024,
                                                    dropout=0.1,
                                                    n_tokens=5)
        self.predict_res_hands = predict_residual_zq(latent_dim=256,
                                                     num_head=8,
                                                     ffn_dim=1024,
                                                     dropout=0.1,
                                                     n_tokens=5)
        self.predict_res_upper = predict_residual_zq(latent_dim=256,
                                                     num_head=8,
                                                     ffn_dim=1024,
                                                     dropout=0.1,
                                                     n_tokens=5)
        self.predict_res_lower = predict_residual_zq(latent_dim=256,
                                                     num_head=8,
                                                     ffn_dim=1024,
                                                     dropout=0.1,
                                                     n_tokens=5)

        ##### con1d #####
        self.face1d = nn.Conv1d(in_channels=256,
                                out_channels=256,
                                kernel_size=2,
                                stride=2)
        self.face1d_2 = nn.Conv1d(in_channels=256,
                                  out_channels=256,
                                  kernel_size=2,
                                  stride=2)
        self.body1d = nn.Conv1d(in_channels=768,
                                out_channels=768,
                                kernel_size=2,
                                stride=2)
        self.upper1d = nn.Conv1d(in_channels=256,
                                 out_channels=256,
                                 kernel_size=2,
                                 stride=2)
        self.hands1d = nn.Conv1d(in_channels=256,
                                 out_channels=256,
                                 kernel_size=2,
                                 stride=2)
        self.lower1d = nn.Conv1d(in_channels=256,
                                 out_channels=256,
                                 kernel_size=2,
                                 stride=2)
        self.spearker_encoder_body1d = nn.Conv1d(in_channels=768,
                                                 out_channels=768,
                                                 kernel_size=2,
                                                 stride=2)
        self.face_latent_mlp = MLP(256, args.hidden_size, 256)

        self.audio_pre_encoder_face = MLP(3, args.hidden_size, 256)
        self.audio_pre_encoder_body = MLP(3, args.hidden_size, 256)
        self.at_attn_face = nn.Linear(args.audio_f * 2, args.audio_f * 2)
        self.at_attn_body = nn.Linear(args.audio_f * 2, args.audio_f * 2)
        args_top = copy.deepcopy(self.args)
        args_top.vae_layer = 3
        args_top.vae_length = args.motion_f  # args.motion_f 256
        args_top.vae_test_dim = 337  # 330 + 3 + 4 = 337
        # self.motion_encoder = MLP(57 * 6, args.hidden_size, 256)
        self.motion_encoder = VQEncoderV6(
            args_top)  # masked motion to latent bs t 337 to bs t 256

        # face decoder  hidden_size:768, audio_f:256, vae_codebook_size:256
        self.feature2face = nn.Linear(args.audio_f * 2,
                                      args.hidden_size)  # 256*2 to 768
        self.face2latent = nn.Linear(
            args.hidden_size, args.vae_codebook_size)  # vae_codebook_size:256
        self.transformer_de_layer = nn.TransformerDecoderLayer(
            d_model=self.args.hidden_size,
            nhead=4,
            dim_feedforward=self.args.hidden_size * 2,
            batch_first=True)

        self.transformer_de_fu_layer = nn.TransformerDecoderLayer(
            d_model=256, nhead=4, dim_feedforward=256 * 2, batch_first=True)
        self.hands_face_decoder = nn.TransformerDecoder(
            self.transformer_de_fu_layer, num_layers=1)
        self.face_hands_decoder = nn.TransformerDecoder(
            self.transformer_de_fu_layer, num_layers=1)

        self.face_decoder = nn.TransformerDecoder(self.transformer_de_layer,
                                                  num_layers=4)
        # pose_length = 64
        self.position_embeddings = PeriodicPositionalEncoding(
            self.args.hidden_size,
            period=self.args.pose_length,
            max_seq_len=self.args.pose_length)

        # motion decoder
        self.transformer_en_layer = nn.TransformerEncoderLayer(
            d_model=self.args.hidden_size,
            nhead=4,
            dim_feedforward=self.args.hidden_size * 2,
            batch_first=True)
        self.motion_self_encoder = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=1)
        self.audio_feature2motion = nn.Linear(args.audio_f,
                                              args.hidden_size)  # 256 to 768
        self.feature2motion = nn.Linear(args.motion_f,
                                        args.hidden_size)  # 256 to 768

        self.bodyhints_face = MLP(args.motion_f, args.hidden_size,
                                  args.motion_f)  # 256 to 256
        self.bodyhints_body = MLP(args.motion_f, args.hidden_size,
                                  args.motion_f)  # 256 to 256
        self.motion2latent_upper = MLP(args.hidden_size, args.hidden_size,
                                       self.args.hidden_size)
        self.motion2latent_hands = MLP(args.hidden_size, args.hidden_size,
                                       self.args.hidden_size)
        self.motion2latent_lower = MLP(args.hidden_size, args.hidden_size,
                                       self.args.hidden_size)

        # 共享 transformer decoder 提取共性 attention map
        decoder_layer_shared = MyTransformerDecoderLayer(d_model=768,
                                                         nhead=4,
                                                         batch_first=True)
        # self.shared_transformer_decoder = MyTransformerDecoder(
        #     decoder_layer_shared, num_layers=1)
        # self.wordhints_decoder = nn.TransformerDecoder(
        #     self.transformer_de_layer, num_layers=8)
        self.wordhints_decoder = MyTransformerDecoder(decoder_layer_shared,
                                                      num_layers=8)
        self.upper_decoder = nn.TransformerDecoder(self.transformer_de_layer,
                                                   num_layers=1)
        self.hands_decoder = nn.TransformerDecoder(self.transformer_de_layer,
                                                   num_layers=1)
        self.lower_decoder = nn.TransformerDecoder(self.transformer_de_layer,
                                                   num_layers=1)

        self.upper_hands_decoder = nn.TransformerDecoder(
            self.transformer_de_fu_layer, num_layers=1)
        self.lower_hands_decoder = nn.TransformerDecoder(
            self.transformer_de_fu_layer, num_layers=1)

        self.face_classifier = MLP(self.args.vae_codebook_size,
                                   args.hidden_size,
                                   self.args.vae_codebook_size)
        self.upper_classifier = MLP(self.args.vae_codebook_size,
                                    args.hidden_size,
                                    self.args.vae_codebook_size)
        self.hands_classifier = MLP(self.args.vae_codebook_size,
                                    args.hidden_size,
                                    self.args.vae_codebook_size)
        self.lower_classifier = MLP(self.args.vae_codebook_size,
                                    args.hidden_size,
                                    self.args.vae_codebook_size)

        self.mask_embeddings = nn.Parameter(torch.zeros(1, 1,
                                                        337))  # [1, 1, 337]
        self.motion_down_upper = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_hands = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_lower = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_upper = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_hands = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.motion_down_lower = nn.Linear(args.hidden_size,
                                           self.args.vae_codebook_size)
        self.moauClip = moauClip(args)
        self.mask_generator = MaskGeneratorMultiConditionFusionSTE(
            motion_dim=337,
            embed_dim=256,
            frame_down_factor=4,
            nhead=4,
            num_layers=1)
        self.joint_dim_reducer = JointDimReducer(num_joints=57,
                                                 in_dim=6,
                                                 out_dim=6)
        self._reset_parameters()

        self.spearker_encoder_body = nn.Embedding(25, args.hidden_size)
        self.spearker_encoder_face = nn.Embedding(25, args.hidden_size)

    def _reset_parameters(self):
        nn.init.normal_(self.mask_embeddings, 0, self.args.hidden_size**-0.5)
    def gen_attention_map(self,hubert=None,
                       beat=None,
                       in_motion=None,
                       use_word=True,
                       in_id=None,):
        cond_face, cond_hands, cond_upper, cond_lower = self.moauClip.get_cond(
                hubert, beat)

        # in_motion [bs, t, j, 6]

        # mask [bs, t, j, 256]

        # masked_motion = self.joint_dim_reducer(masked_motion)
        # masked_motion = masked_motion.reshape(N, T, J * 6)
        body_hint = self.motion_encoder(in_motion)  # bs t 256
        # print('speaker_embedding_face:', speaker_embedding_face.shape)
        # print('speaker_embedding_face:', speaker_embedding_face.shape)
        speaker_embedding_body = self.spearker_encoder_body(in_id).squeeze(2)

        body_hint_body = self.bodyhints_body(body_hint)  # MLP 256 to 256
        motion_embeddings = self.feature2motion(
            body_hint_body)  # linear 256 to 768
        motion_embeddings = speaker_embedding_body + motion_embeddings  # bs, t, 768
        motion_embeddings = self.position_embeddings(
            motion_embeddings)  # bs, t, 768
        motion_refined_embeddings = self.motion_self_encoder(motion_embeddings)

        # audio to gesture cross-modal attention
        if use_word:
            a2g_motion_hands = self.audio_feature2motion_hands(
                cond_hands)  # linear 256 to 768
            a2g_motion_upper = self.audio_feature2motion_upper(
                cond_upper)  # linear 256 to 768
            a2g_motion_lower = self.audio_feature2motion_lower(
                cond_lower)  # linear 256 to 768
            motion_refined_embeddings_in = motion_refined_embeddings + speaker_embedding_body  # bs, t, 768
            motion_refined_embeddings_in = self.position_embeddings(
                motion_refined_embeddings)
            cond = {
                "hands": a2g_motion_hands,
                "upper": a2g_motion_upper,
                "lower": a2g_motion_lower
            }
            attention_map = self.mask_generator.get_attention_map(
                motion=motion_refined_embeddings_in,
                conditions=cond,
                need_weights=True)
        return attention_map, cond_hands
    def gen_mask_frame(self,
                       hubert=None,
                       beat=None,
                       face_zq=None,
                       hands_zq=None,
                       upper_zq=None,
                       lower_zq=None,
                       mask=None,
                       mask_ratio=None,
                       hard_ratio=None,
                       soft_ratio=None,
                       is_test=None,
                       in_motion=None,
                       use_attentions=True,
                       use_word=True,
                       in_id=None,
                       train_mode=True,
                       hard_mask=False):

        in_word_body = self.hubert_encoder_body(hubert.permute(0, 2,
                                                               1)).permute(
                                                                   0, 2, 1)
        # in_word_body = self.text_encoder_body(in_word_body)
        bs, t, c = in_word_body.shape
        in_audio_body = self.audio_pre_encoder_body(beat)  # [bs, t, 256]

        if use_attentions:
            alpha_at_body = torch.cat([in_word_body, in_audio_body],
                                      dim=-1).reshape(bs, t, c * 2)
            alpha_at_body = self.at_attn_body(alpha_at_body).reshape(
                bs, t, c, 2)
            alpha_at_body = alpha_at_body.softmax(dim=-1)
            fusion_body = in_word_body * alpha_at_body[:, :, :,
                                                       1] + in_audio_body * alpha_at_body[:, :, :,
                                                                                          0]
        else:
            fusion_body = in_word_body + in_audio_body

        # in_motion [bs, t, j, 6]

        # mask [bs, t, j, 256]

        # masked_motion = self.joint_dim_reducer(masked_motion)
        # masked_motion = masked_motion.reshape(N, T, J * 6)
        body_hint = self.motion_encoder(in_motion)  # bs t 256
        # print('speaker_embedding_face:', speaker_embedding_face.shape)
        # print('speaker_embedding_face:', speaker_embedding_face.shape)
        speaker_embedding_body = self.spearker_encoder_body(in_id).squeeze(2)

        body_hint_body = self.bodyhints_body(body_hint)  # MLP 256 to 256
        motion_embeddings = self.feature2motion(
            body_hint_body)  # linear 256 to 768
        motion_embeddings = speaker_embedding_body + motion_embeddings  # bs, t, 768
        motion_embeddings = self.position_embeddings(
            motion_embeddings)  # bs, t, 768
        motion_refined_embeddings = self.motion_self_encoder(motion_embeddings)

        # audio to gesture cross-modal attention
        if use_word:
            a2g_motion = self.audio_feature2motion(
                fusion_body)  # linear 256 to 768
            motion_refined_embeddings_in = motion_refined_embeddings + speaker_embedding_body  # bs, t, 768
            motion_refined_embeddings_in = self.position_embeddings(
                motion_refined_embeddings)
            word_hints, cross_attention_map = self.wordhints_decoder(
                tgt=motion_refined_embeddings_in,
                memory=a2g_motion,
                need_weights=True)
            cross_attention_map = cross_attention_map.mean(dim=(0,
                                                                2))  # bs, t\
        mask_frame = Soft2HardMaskSTE.apply(cross_attention_map, mask_ratio,
                                            hard_ratio, soft_ratio)
        return mask_frame

    def forward(self,
                hubert=None,
                beat=None,
                face_zq=None,
                hands_zq=None,
                upper_zq=None,
                lower_zq=None,
                mask=None,
                mask_frame=None,
                mask_ratio=None,
                hard_ratio=None,
                soft_ratio=None,
                is_test=None,
                in_motion=None,
                use_attentions=True,
                use_word=True,
                in_id=None,
                train_mode=True,
                hard_mask=False):
        # if train_mode:
        #     cond_face, cond_hands, cond_upper, cond_lower, loss_moau = self.moauClip(
        #         hubert, beat, face_zq, hands_zq, upper_zq, lower_zq)
        # else:
        #     cond_face, cond_hands, cond_upper, cond_lower = self.moauClip.get_cond(
        #         hubert, beat)
        #     loss_moau = torch.tensor(0.0)
        t_mmm_start = time.time()
        loss_moau = torch.tensor(0.0)

        in_word_face = self.hubert_encoder(hubert.permute(0, 2,
                                                          1)).permute(0, 2, 1)
        in_word_body = self.hubert_encoder_body(hubert.permute(0, 2,
                                                               1)).permute(
                                                                   0, 2, 1)
        # in_word_body = self.text_encoder_body(in_word_body)
        bs, t, c = in_word_face.shape
        in_audio_face = self.audio_pre_encoder_face(beat)  # [bs, t, 256]
        in_audio_body = self.audio_pre_encoder_body(beat)  # [bs, t, 256]

        if use_attentions:
            alpha_at_face = torch.cat([in_word_face, in_audio_face],
                                      dim=-1).reshape(bs, t, c * 2)
            alpha_at_face = self.at_attn_face(alpha_at_face).reshape(
                bs, t, c, 2)  # bs, t, c, 2
            alpha_at_face = alpha_at_face.softmax(dim=-1)
            fusion_face = in_word_face * alpha_at_face[:, :, :,
                                                       1] + in_audio_face * alpha_at_face[:, :, :,
                                                                                          0]
            alpha_at_body = torch.cat([in_word_body, in_audio_body],
                                      dim=-1).reshape(bs, t, c * 2)
            alpha_at_body = self.at_attn_body(alpha_at_body).reshape(
                bs, t, c, 2)
            alpha_at_body = alpha_at_body.softmax(dim=-1)
            fusion_body = in_word_body * alpha_at_body[:, :, :,
                                                       1] + in_audio_body * alpha_at_body[:, :, :,
                                                                                          0]
        else:
            fusion_face = in_word_face + in_audio_face
            fusion_body = in_word_body + in_audio_body

        # in_motion [bs, t, j, 6]

        # mask [bs, t, j, 256]

        if hard_mask:
            mask_frame = mask_frame.unsqueeze(-1).repeat(1, 1, 337)
            masked_embeddings = self.mask_embeddings.expand_as(in_motion)
            masked_motion = torch.where(mask_frame == 1, masked_embeddings,
                                        in_motion)
            body_hint = self.motion_encoder(masked_motion)
        else:
            # N, T, JC = in_motion.shape
            masked_embeddings = self.mask_embeddings.expand_as(
                in_motion)  # in_motion [bs, t, j, 6]
            # mask [bs, t, j, 6], 前四帧为0，后面为1
            masked_motion = torch.where(mask == 1, masked_embeddings,
                                        in_motion)
            # masked_motion = self.joint_dim_reducer(masked_motion)
            # masked_motion = masked_motion.reshape(N, T, J * 6)
            body_hint = self.motion_encoder(masked_motion)  # bs t 256
        speaker_embedding_face = self.spearker_encoder_face(in_id).squeeze(
            2)  # bs, t, 768
        # print('speaker_embedding_face:', speaker_embedding_face.shape)
        # print('speaker_embedding_face:', speaker_embedding_face.shape)
        speaker_embedding_body = self.spearker_encoder_body(in_id).squeeze(2)

        # decode face
        use_body_hints = True
        if use_body_hints:
            body_hint_face = self.bodyhints_face(body_hint)
            fusion_face_a = torch.cat([fusion_face, body_hint_face], dim=2)
        a2g_face = self.feature2face(fusion_face_a)
        face_embeddings = speaker_embedding_face
        face_embeddings = self.position_embeddings(face_embeddings)
        # print('face_embeddings:', face_embeddings.shape)
        decoded_face = self.face_decoder(tgt=face_embeddings, memory=a2g_face)
        face_latent = self.face2latent(decoded_face)
        face_latent = face_latent.permute(0, 2, 1)
        face_latent = self.face1d(face_latent)
        face_latent = face_latent.permute(0, 2, 1)
        face_latent = self.face_latent_mlp(face_latent)
        face_latent = face_latent.permute(0, 2, 1)
        face_latent = self.face1d_2(face_latent)
        # face_latent_cls = face_latent.permute(0, 2, 1)
        # print('face_latent_cls:', face_latent_cls.shape)
        # exit()
        face_latent_prezq = face_latent.permute(0, 2, 1)
        ######### hubert cons loss ###########
        hubert_cons_loss = self.hubert_face_cons_loss(face_latent_prezq,
                                                      in_word_face)
        # beat_face_cons_loss = self.beat_face_cons_loss(face_latent_prezq, in_audio_face)
        # face_latent_prezq = self.face_cons_mlp(face_latent_prezq)

        body_hint_body = self.bodyhints_body(body_hint)  # MLP 256 to 256
        motion_embeddings = self.feature2motion(
            body_hint_body)  # linear 256 to 768
        motion_embeddings = speaker_embedding_body + motion_embeddings  # bs, t, 768
        motion_embeddings = self.position_embeddings(
            motion_embeddings)  # bs, t, 768

        # bi-directional self-attention
        motion_refined_embeddings = self.motion_self_encoder(motion_embeddings)

        # audio to gesture cross-modal attention
        if use_word:
            a2g_motion = self.audio_feature2motion(
                fusion_body)  # linear 256 to 768
            motion_refined_embeddings_in = motion_refined_embeddings + speaker_embedding_body  # bs, t, 768
            motion_refined_embeddings_in = self.position_embeddings(
                motion_refined_embeddings)
            word_hints = self.wordhints_decoder(
                tgt=motion_refined_embeddings_in, memory=a2g_motion)
            motion_refined_embeddings = motion_refined_embeddings + word_hints

        # feedforward
        motion_refined_embeddings = motion_refined_embeddings.permute(0, 2, 1)
        motion_refined_embeddings = self.body1d(motion_refined_embeddings)
        motion_refined_embeddings = motion_refined_embeddings.permute(0, 2, 1)
        speaker_embedding_body = speaker_embedding_body.permute(0, 2, 1)
        speaker_embedding_body = self.spearker_encoder_body1d(
            speaker_embedding_body)
        speaker_embedding_body = speaker_embedding_body.permute(0, 2, 1)

        upper_latent = self.motion2latent_upper(motion_refined_embeddings)
        hands_latent = self.motion2latent_hands(motion_refined_embeddings)
        lower_latent = self.motion2latent_lower(motion_refined_embeddings)

        upper_latent_in = upper_latent + speaker_embedding_body
        upper_latent_in = self.position_embeddings(upper_latent_in)
        hands_latent_in = hands_latent + speaker_embedding_body
        hands_latent_in = self.position_embeddings(hands_latent_in)
        lower_latent_in = lower_latent + speaker_embedding_body
        lower_latent_in = self.position_embeddings(lower_latent_in)

        # transformer decoder
        motion_upper = self.upper_decoder(tgt=upper_latent_in,
                                          memory=hands_latent + lower_latent)
        motion_hands = self.hands_decoder(tgt=hands_latent_in,
                                          memory=upper_latent + lower_latent)
        motion_lower = self.lower_decoder(tgt=lower_latent_in,
                                          memory=upper_latent + hands_latent)
        upper_latent = self.motion_down_upper(
            motion_upper + upper_latent)  # linear 768 to 256
        hands_latent = self.motion_down_hands(motion_hands + hands_latent)
        lower_latent = self.motion_down_lower(motion_lower + lower_latent)

        upper_latent = upper_latent.permute(0, 2, 1)
        upper_latent = self.upper1d(upper_latent)
        upper_latent_prezq = upper_latent.permute(0, 2, 1)

        hands_latent = hands_latent.permute(0, 2, 1)
        hands_latent = self.hands1d(hands_latent)
        hands_latent_prezq = hands_latent.permute(0, 2, 1)

        ########## beat cons loss ###########
        beat_cons_loss = self.beat_cons_loss(hands_latent_prezq, in_word_body)
        # hubert_cons_loss = self.hubert_cons_loss(hands_latent_prezq, in_word_body )
        # beat_cons_loss = self.beat_cons_loss(hands_latent_prezq, in_audio_body)

        # hands_latent_prezq = self.hands_cons_mlp(hands_latent_prezq)

        lower_latent = lower_latent.permute(0, 2, 1)
        lower_latent = self.lower1d(lower_latent)
        lower_latent_prezq = lower_latent.permute(0, 2, 1)

        hands_latent = self.hands_face_decoder(tgt=hands_latent_prezq,
                                               memory=face_latent_prezq)
        face_latent = self.face_hands_decoder(tgt=face_latent_prezq,
                                              memory=hands_latent_prezq)
        # face_latent = face_latent_prezq
        upper_latent = self.upper_hands_decoder(tgt=upper_latent_prezq,
                                                memory=hands_latent +
                                                lower_latent_prezq)
        lower_latent = self.lower_hands_decoder(tgt=lower_latent_prezq,
                                                memory=upper_latent_prezq +
                                                hands_latent)
        # face_latent = face_latent_prezq
        # upper_latent = upper_latent_prezq
        # lower_latent = lower_latent_prezq
        # hands_latent = hands_latent_prezq

        # ########### beat cons loss ###########
        # beat_cons_loss = self.beat_cons_loss(hands_latent, in_word_body)
        # ########## hubert cons loss ###########
        # hubert_cons_loss = self.hubert_cons_loss(face_latent, in_word_face)

        zq_index0_lower = self.lower_classifier(lower_latent)
        # zq0_lower = self.lower_classifier(lower_latent)
        zq_index0_face = self.face_classifier(face_latent)  # bs, t, 256
        # zq0_face = self.face_classifier(face_latent) # bs, t, 256

        zq1_face, zq2_face, zq3_face, zq4_face, zq5_face, zq_index1_face, zq_index2_face, zq_index3_face, zq_index4_face, zq_index5_face = self.predict_res_face(
            face_latent, fusion_face)
        # motion spatial encoder
        zq1_lower, zq2_lower, zq3_lower, zq4_lower, zq5_lower, zq_index1_lower, zq_index2_lower, zq_index3_lower, zq_index4_lower, zq_index5_lower = self.predict_res_lower(
            lower_latent, fusion_body)

        zq_index0_upper = self.upper_classifier(upper_latent)
        # zq0_upper = self.upper_classifier(upper_latent)
        zq1_upper, zq2_upper, zq3_upper, zq4_upper, zq5_upper, zq_index1_upper, zq_index2_upper, zq_index3_upper, zq_index4_upper, zq_index5_upper = self.predict_res_upper(
            upper_latent, fusion_body)

        zq_index0_hands = self.hands_classifier(hands_latent)
        # zq0_hands = self.hands_classifier(hands_latent)
        zq1_hands, zq2_hands, zq3_hands, zq4_hands, zq5_hands, zq_index1_hands, zq_index2_hands, zq_index3_hands, zq_index4_hands, zq_index5_hands = self.predict_res_hands(
            hands_latent, fusion_body)

        cls_face = torch.stack([
            zq_index0_face, zq_index1_face, zq_index2_face, zq_index3_face,
            zq_index4_face, zq_index5_face
        ],
                               dim=-1)
        # cls_face = zq_index0_face

        cls_upper = torch.stack([
            zq_index0_upper, zq_index1_upper, zq_index2_upper, zq_index3_upper,
            zq_index4_upper, zq_index5_upper
        ],
                                dim=-1)
        cls_lower = torch.stack([
            zq_index0_lower, zq_index1_lower, zq_index2_lower, zq_index3_lower,
            zq_index4_lower, zq_index5_lower
        ],
                                dim=-1)
        cls_hands = torch.stack([
            zq_index0_hands, zq_index1_hands, zq_index2_hands, zq_index3_hands,
            zq_index4_hands, zq_index5_hands
        ],
                                dim=-1)

        # rec_face = face_latent.unsqueeze(2)
        rec_face = torch.stack(
            [face_latent, zq1_face, zq2_face, zq3_face, zq4_face, zq5_face],
            dim=1).unsqueeze(2)

        rec_upper = torch.stack([
            upper_latent, zq1_upper, zq2_upper, zq3_upper, zq4_upper, zq5_upper
        ],
                                dim=1).unsqueeze(2)
        rec_lower = torch.stack([
            lower_latent, zq1_lower, zq2_lower, zq3_lower, zq4_lower, zq5_lower
        ],
                                dim=1).unsqueeze(2)
        rec_hands = torch.stack([
            hands_latent, zq1_hands, zq2_hands, zq3_hands, zq4_hands, zq5_hands
        ],
                                dim=1).unsqueeze(2)
        # hubert_cons_loss = hubert_cons_loss + hubert_face_cons_loss
        # beat_cons_loss = beat_cons_loss + beat_face_cons_loss
        return {
            'hubert_cons_loss': hubert_cons_loss,
            'beat_cons_loss': beat_cons_loss,
            "rec_face": rec_face,
            "rec_upper": rec_upper,
            "rec_lower": rec_lower,
            "rec_hands": rec_hands,
            "loss_moau": loss_moau,
            "cls_face": cls_face,
            "cls_upper": cls_upper,
            "cls_lower": cls_lower,
            "cls_hands": cls_hands,
        }


class moauClip(nn.Module):

    def __init__(self, args=None):
        super(moauClip, self).__init__()
        self.args = args
        self.moau_query_face = nn.Parameter(torch.zeros(1, 1, 256))
        self.moau_query_hands = nn.Parameter(torch.zeros(1, 1, 256))
        self.moau_query_upper = nn.Parameter(torch.zeros(1, 1, 256))
        self.moau_query_lower = nn.Parameter(torch.zeros(1, 1, 256))
        self.hubert_encoder_face = nn.Sequential(*[
            nn.Conv1d(1024, 256, 3, 1, 1, bias=False),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.Conv1d(256, 256, 3, 1, 1, bias=False)
        ])
        self.hubert_encoder_body = nn.Sequential(*[
            nn.Conv1d(1024, 256, 3, 1, 1, bias=False),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.Conv1d(256, 256, 3, 1, 1, bias=False)
        ])
        self.audio_pre_encoder_face = MLP(3, args.hidden_size, 256)
        self.audio_pre_encoder_body = MLP(3, args.hidden_size, 256)

        self.hubert_cons = RhythmicIdentificationLoss(temperature=0.1)
        self.beat_cons = RhythmicIdentificationLoss(temperature=0.1)
        self.clip_cons = RhythmicIdentificationLoss(temperature=0.1)
        self.transformer_en_layer = nn.TransformerEncoderLayer(
            d_model=256, nhead=4, dim_feedforward=512, batch_first=True)
        self.motion_encoder_face_1 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_hands_1 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_upper_1 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_lower_1 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_face_2 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_hands_2 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_upper_2 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_lower_2 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_face_3 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_hands_3 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_upper_3 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.motion_encoder_lower_3 = nn.TransformerEncoder(
            self.transformer_en_layer, num_layers=2)
        self.transformer_de_layer = nn.TransformerDecoderLayer(
            d_model=256, nhead=4, dim_feedforward=512, batch_first=True)
        self.face_decoder_low = nn.TransformerDecoder(
            self.transformer_de_layer, num_layers=1)
        self.hands_decoder_low = nn.TransformerDecoder(
            self.transformer_de_layer, num_layers=1)
        self.upper_decoder_low = nn.TransformerDecoder(
            self.transformer_de_layer, num_layers=1)
        self.lower_decoder_low = nn.TransformerDecoder(
            self.transformer_de_layer, num_layers=1)
        self.face_decoder_high = nn.TransformerDecoder(
            self.transformer_de_layer, num_layers=1)
        self.hands_decoder_high = nn.TransformerDecoder(
            self.transformer_de_layer, num_layers=1)
        self.upper_decoder_high = nn.TransformerDecoder(
            self.transformer_de_layer, num_layers=1)
        self.lower_decoder_high = nn.TransformerDecoder(
            self.transformer_de_layer, num_layers=1)
        self.predict_motion_face = nn.Linear(256, 256)
        self.predict_motion_hands = nn.Linear(256, 256)
        self.predict_motion_upper = nn.Linear(256, 256)
        self.predict_motion_lower = nn.Linear(256, 256)
        self.position_embeddings = PeriodicPositionalEncoding(
            256,
            period=self.args.pose_length,
            max_seq_len=self.args.pose_length)
        self.loss_fn = nn.MSELoss()

    def forward(self,
                hubert,
                beat,
                face_zq,
                hands_zq,
                upper_zq,
                lower_zq,
                circle_loss=True):
        # face_zq [bs, 6, 1, 16, 256]
        face_zq = face_zq.sum(dim=1).squeeze(1)  # bs, t, 256
        hands_zq = hands_zq.sum(dim=1).squeeze(1)  # bs, t, 256
        upper_zq = upper_zq.sum(dim=1).squeeze(1)  # bs, t, 256
        lower_zq = lower_zq.sum(dim=1).squeeze(1)  # bs, t, 256
        in_high_face = self.hubert_encoder_face(hubert.permute(0, 2,
                                                               1)).permute(
                                                                   0, 2, 1)
        in_high_body = self.hubert_encoder_body(hubert.permute(0, 2,
                                                               1)).permute(
                                                                   0, 2, 1)
        bs, t, c = in_high_face.shape
        in_low_face = self.audio_pre_encoder_face(beat)  # [bs, t, 256]
        in_low_body = self.audio_pre_encoder_body(beat)  # [bs, t, 256]
        hubert_cons_face = self.hubert_cons(face_zq, in_high_face)
        beat_cons_face = self.beat_cons(face_zq, in_low_face)
        hubert_cons_hands = self.hubert_cons(hands_zq, in_high_body)
        beat_cons_hands = self.beat_cons(hands_zq, in_low_body)
        # hubert_cons_upper = self.hubert_cons(upper_zq, in_high)
        # beat_cons_upper = self.beat_cons(upper_zq, in_low)
        # hubert_cons_lower = self.hubert_cons(lower_zq, in_high)
        # beat_cons_lower = self.beat_cons(lower_zq, in_low)
        moau_query_face = self.moau_query_face.repeat(bs, 64, 1)
        moau_query_hands = self.moau_query_hands.repeat(bs, 64, 1)
        moau_query_upper = self.moau_query_upper.repeat(bs, 64, 1)
        moau_query_lower = self.moau_query_lower.repeat(bs, 64, 1)

        in_high_face = self.position_embeddings(in_high_face)
        in_high_body = self.position_embeddings(in_high_body)
        in_low_face = self.position_embeddings(in_low_face)
        in_low_body = self.position_embeddings(in_low_body)
        l_motion_face = self.position_embeddings(face_zq)
        l_motion_hands = self.position_embeddings(hands_zq)
        l_motion_upper = self.position_embeddings(upper_zq)
        l_motion_lower = self.position_embeddings(lower_zq)
        out_motion_face = self.motion_encoder_face_1(l_motion_face)
        out_motion_hands = self.motion_encoder_hands_1(l_motion_hands)
        out_motion_upper = self.motion_encoder_upper_1(l_motion_upper)
        out_motion_lower = self.motion_encoder_lower_1(l_motion_lower)
        out_motion_face = self.motion_encoder_face_2(out_motion_face)
        out_motion_hands = self.motion_encoder_hands_2(out_motion_hands)
        out_motion_upper = self.motion_encoder_upper_2(out_motion_upper)
        out_motion_lower = self.motion_encoder_lower_2(out_motion_lower)
        out_motion_face = self.motion_encoder_face_3(out_motion_face)
        out_motion_hands = self.motion_encoder_hands_3(out_motion_hands)
        out_motion_upper = self.motion_encoder_upper_3(out_motion_upper)
        out_motion_lower = self.motion_encoder_lower_3(out_motion_lower)

        moau_query_face = self.position_embeddings(moau_query_face)
        moau_query_hands = self.position_embeddings(moau_query_hands)
        moau_query_upper = self.position_embeddings(moau_query_upper)
        moau_query_lower = self.position_embeddings(moau_query_lower)
        q_face_1 = self.motion_encoder_face_1(moau_query_face)
        q_hands_1 = self.motion_encoder_hands_1(moau_query_hands)
        q_upper_1 = self.motion_encoder_upper_1(moau_query_upper)
        q_lower_1 = self.motion_encoder_lower_1(moau_query_lower)

        q_face_2_in = self.face_decoder_low(tgt=q_face_1, memory=in_low_face)
        q_hands_2_in = self.hands_decoder_low(tgt=q_hands_1,
                                              memory=in_low_body)
        q_upper_2_in = self.upper_decoder_low(tgt=q_upper_1,
                                              memory=in_low_body)
        q_lower_2_in = self.lower_decoder_low(tgt=q_lower_1,
                                              memory=in_low_body)

        q_face_2 = self.motion_encoder_face_2(q_face_2_in)
        q_hands_2 = self.motion_encoder_hands_2(q_hands_2_in)
        q_upper_2 = self.motion_encoder_upper_2(q_upper_2_in)
        q_lower_2 = self.motion_encoder_lower_2(q_lower_2_in)

        q_face_3_in = self.face_decoder_high(tgt=q_face_2, memory=in_high_face)
        q_hands_3_in = self.hands_decoder_high(tgt=q_hands_2,
                                               memory=in_high_body)
        q_upper_3_in = self.upper_decoder_high(tgt=q_upper_2,
                                               memory=in_high_body)
        q_lower_3_in = self.lower_decoder_high(tgt=q_lower_2,
                                               memory=in_high_body)

        q_face = self.motion_encoder_face_3(q_face_3_in)
        q_hands = self.motion_encoder_hands_3(q_hands_3_in)
        q_upper = self.motion_encoder_upper_3(q_upper_3_in)
        q_lower = self.motion_encoder_lower_3(q_lower_3_in)

        q_face_clip = self.clip_cons(out_motion_face, q_face)
        q_hands_clip = self.clip_cons(out_motion_hands, q_hands)
        q_upper_clip = self.clip_cons(out_motion_upper, q_upper)
        q_lower_clip = self.clip_cons(out_motion_lower, q_lower)
        if circle_loss:
            q_face = self.predict_motion_face(q_face)
            q_hands = self.predict_motion_hands(q_hands)
            q_upper = self.predict_motion_upper(q_upper)
            q_lower = self.predict_motion_lower(q_lower)
            q_face_r = q_face.reshape(bs, 16, 4, 256).mean(dim=2)
            q_hands_r = q_hands.reshape(bs, 16, 4, 256).mean(dim=2)
            q_upper_r = q_upper.reshape(bs, 16, 4, 256).mean(dim=2)
            q_lower_r = q_lower.reshape(bs, 16, 4, 256).mean(dim=2)
            q_face_loss = self.loss_fn(q_face_r, out_motion_face)
            q_hands_loss = self.loss_fn(q_hands_r, out_motion_hands)
            q_upper_loss = self.loss_fn(q_upper_r, out_motion_upper)
            q_lower_loss = self.loss_fn(q_lower_r, out_motion_lower)
        else:
            q_face_loss = torch.tensor(0)
            q_hands_loss = torch.tensor(0)
            q_upper_loss = torch.tensor(0)
            q_lower_loss = torch.tensor(0)
        hubert_cons_upper = torch.tensor(0)
        beat_cons_upper = torch.tensor(0)
        hubert_cons_lower = torch.tensor(0)
        beat_cons_lower = torch.tensor(0)
        loss_con = hubert_cons_face + beat_cons_face + hubert_cons_hands + beat_cons_hands + hubert_cons_upper + beat_cons_upper + hubert_cons_lower + beat_cons_lower
        loss_q = q_face_loss + q_hands_loss + q_upper_loss + q_lower_loss
        loss_clip = q_face_clip + q_hands_clip + q_upper_clip + q_lower_clip
        loss_all = loss_con + loss_q + loss_clip
        return q_face, q_hands, q_upper, q_lower, loss_all
        # return {
        #     "hubert_cons_face": hubert_cons_face,
        #     "beat_cons_face": beat_cons_face,
        #     "hubert_cons_hands": hubert_cons_hands,
        #     "beat_cons_hands": beat_cons_hands,
        #     "hubert_cons_upper": hubert_cons_upper,
        #     "beat_cons_upper": beat_cons_upper,
        #     "hubert_cons_lower": hubert_cons_lower,
        #     "beat_cons_lower": beat_cons_lower,
        #     "q_face_loss": q_face_loss,
        #     "q_hands_loss": q_hands_loss,
        #     "q_upper_loss": q_upper_loss,
        #     "q_lower_loss": q_lower_loss,
        #     "q_face_clip": q_face_clip,
        #     "q_hands_clip": q_hands_clip,
        #     "q_upper_clip": q_upper_clip,
        #     "q_lower_clip": q_lower_clip,
        # }

    def get_cond(self, hubert, beat):
        in_high_face = self.hubert_encoder_face(hubert.permute(0, 2,
                                                               1)).permute(
                                                                   0, 2, 1)
        in_high_body = self.hubert_encoder_body(hubert.permute(0, 2,
                                                               1)).permute(
                                                                   0, 2, 1)
        bs, t, c = in_high_face.shape
        in_low_face = self.audio_pre_encoder_face(beat)  # [bs, t, 256]
        in_low_body = self.audio_pre_encoder_body(beat)  # [bs, t, 256]
        moau_query_face = self.moau_query_face.repeat(bs, 64, 1)
        moau_query_hands = self.moau_query_hands.repeat(bs, 64, 1)
        moau_query_upper = self.moau_query_upper.repeat(bs, 64, 1)
        moau_query_lower = self.moau_query_lower.repeat(bs, 64, 1)

        in_high_face = self.position_embeddings(in_high_face)
        in_high_body = self.position_embeddings(in_high_body)
        in_low_face = self.position_embeddings(in_low_face)
        in_low_body = self.position_embeddings(in_low_body)

        moau_query_face = self.position_embeddings(moau_query_face)
        moau_query_hands = self.position_embeddings(moau_query_hands)
        moau_query_upper = self.position_embeddings(moau_query_upper)
        moau_query_lower = self.position_embeddings(moau_query_lower)
        q_face_1 = self.motion_encoder_face_1(moau_query_face)
        q_hands_1 = self.motion_encoder_hands_1(moau_query_hands)
        q_upper_1 = self.motion_encoder_upper_1(moau_query_upper)
        q_lower_1 = self.motion_encoder_lower_1(moau_query_lower)

        q_face_2_in = self.face_decoder_low(tgt=q_face_1, memory=in_low_face)
        q_hands_2_in = self.hands_decoder_low(tgt=q_hands_1,
                                              memory=in_low_body)
        q_upper_2_in = self.upper_decoder_low(tgt=q_upper_1,
                                              memory=in_low_body)
        q_lower_2_in = self.lower_decoder_low(tgt=q_lower_1,
                                              memory=in_low_body)

        q_face_2 = self.motion_encoder_face_2(q_face_2_in)
        q_hands_2 = self.motion_encoder_hands_2(q_hands_2_in)
        q_upper_2 = self.motion_encoder_upper_2(q_upper_2_in)
        q_lower_2 = self.motion_encoder_lower_2(q_lower_2_in)

        q_face_3_in = self.face_decoder_high(tgt=q_face_2, memory=in_high_face)
        q_hands_3_in = self.hands_decoder_high(tgt=q_hands_2,
                                               memory=in_high_body)
        q_upper_3_in = self.upper_decoder_high(tgt=q_upper_2,
                                               memory=in_high_body)
        q_lower_3_in = self.lower_decoder_high(tgt=q_lower_2,
                                               memory=in_high_body)

        q_face = self.motion_encoder_face_3(q_face_3_in)
        q_hands = self.motion_encoder_hands_3(q_hands_3_in)
        q_upper = self.motion_encoder_upper_3(q_upper_3_in)
        q_lower = self.motion_encoder_lower_3(q_lower_3_in)

        return q_face, q_hands, q_upper, q_lower


class InputProcess(nn.Module):

    def __init__(self, input_feats, latent_dim):
        super().__init__()
        self.input_feats = input_feats
        self.latent_dim = latent_dim
        self.poseEmbedding = nn.Linear(self.input_feats, self.latent_dim)

    def forward(self, x):
        # [bs, ntokens, input_feats]
        # x = x.permute((1, 0, 2)) # [seqen, bs, input_feats]
        # print(x.shape)
        x = self.poseEmbedding(x)  # [seqlen, bs, d]
        return x


class PositionalEncoding(nn.Module):
    #Borrow from MDM, the same as above, but add dropout, exponential may improve precision
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)  #[max_len, 1, d_model]

        self.register_buffer('pe', pe)

    def forward(self, x):
        # not used in the final model
        x = x + self.pe[:x.shape[0], :]
        return self.dropout(x)


class PositionalEncoding2D(nn.Module):

    def __init__(self, d_model, dropout=0.1, height=200, width=50):
        super(PositionalEncoding2D, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        if d_model % 4 != 0:
            raise ValueError("Cannot use sin/cos positional encoding with "
                             "odd dimension (got dim={:d})".format(d_model))
        pe = torch.zeros(d_model, height, width)
        # Each dimension use half of d_model
        d_model = int(d_model / 2)
        div_term = torch.exp(
            torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))
        pos_w = torch.arange(0., width).unsqueeze(1)
        pos_h = torch.arange(0., height).unsqueeze(1)
        pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(
            0, 1).unsqueeze(1).repeat(1, height, 1)
        pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(
            0, 1).unsqueeze(1).repeat(1, height, 1)
        pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(
            0, 1).unsqueeze(2).repeat(1, 1, width)
        pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(
            0, 1).unsqueeze(2).repeat(1, 1, width)
        pe = pe.permute(1, 2, 0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.shape[0], :x.shape[1], None, :]
        return self.dropout(x)


if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    code_index = torch.randint(0, 256, (1, 16, 8)).to(device)
    cond = ['torch.randn(2)', 'torch.randn(2)']
    cond = np.array(cond)
    m_length = torch.tensor(16).repeat(1).to(device)
    model = MaskTransformer2D(
        code_dim=256,
        cond_mode='speech',
        latent_dim=256,
        ff_size=1024,
        num_layers=4,
        num_heads=8,
        dropout=0.1,
        clip_dim=256,
        cond_drop_prob=0.1,
    )
    model.to(device)
    hubert = torch.randn(1, 64, 1024).to(device)
    beat = torch.randn(1, 64, 3).to(device)
    model(hubert, beat, code_index, m_length)
    captions = 'the man is running in a circle.'
    token_lens = torch.tensor(16).unsqueeze(0).to(device)
    gen_index = model.generate(hubert,
                               beat,
                               token_lens,
                               n_j=8,
                               timesteps=18,
                               cond_scale=4,
                               temperature=1,
                               topk_filter_thres=0.9,
                               gsample=False)
    gen_index = gen_index.cpu().numpy()
    print(gen_index.shape)
    # exit()
    all_indices = torch.randint(0, 256, (1, 16, 8, 6)).to(device)

    m_length_res = torch.tensor(16).repeat(1).to(device)
    res_model = ResidualTransformer2D(
        code_dim=256,
        cond_mode='speech',
        latent_dim=256,
        ff_size=1024,
        num_layers=4,
        num_heads=8,
        dropout=0.1,
        clip_dim=256,
        shared_codebook=False,
        cond_drop_prob=0.2,
        # codebook=vq_model.quantizer.codebooks[0] if opt.fix_token_emb else None,
        share_weight=True,
    )
    res_model.to(device)
    res_model(hubert, beat, all_indices, m_length_res)
    print('gen_index.shape:', gen_index.shape)
    print('captions:', captions)
    gen_index = torch.tensor(gen_index).to(device)
    all_tokens = res_model.generate(hubert,
                                    beat,
                                    gen_index,
                                    token_lens,
                                    temperature=1,
                                    cond_scale=5)
    kst2_model = kst2_2d(
        code_dim=256,
        cond_mode='speech',
        latent_dim=256,
        ff_size=1024,
    )
    kst2_model.to(device)
    hands_index = torch.randint(0, 256, (1, 16, 8, 6)).to(device)
    upper_index = torch.randint(0, 256, (1, 16, 4, 6)).to(device)
    lower_index = torch.randint(0, 256, (1, 16, 3, 6)).to(device)

    kst2_model(hubert, beat, hands_index, upper_index, lower_index,
               m_length_res)
    kst2_model.generate(hubert, beat, m_length_res)
